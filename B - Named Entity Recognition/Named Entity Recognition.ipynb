{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling and NER \n",
    "\n",
    "## B) Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this jupyter notebook, the folder \"gmb-2.2.0\", which contains all the files needed, should exist in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the extraction of the Named Entities from the dataset we make use of the following functions.\n",
    "\n",
    "The function read_gmb() reads the files, splits the sentences by 2 newline characters, the words by 1 newline character and the annotation by tab character, gets rid of the sub-categories and then for the tuples of the form (word, tag, ner) it has created, calls the to_conll_iob() function. \n",
    "\n",
    "The to_conll_iob() function reforms the label into the IOB representation so as to be used in the training of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    " \n",
    "\n",
    "def read_gmb(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            if tag in ('LQU', 'RQU'):   \n",
    "                                tag = \"``\"\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    "                        yield [((w, t), iob) for w, t, iob in conll_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_gmb() function\n",
    "corpus_root = \"gmb-2.2.0\"\n",
    "reader = read_gmb(corpus_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the output of the above function into a list of tuples to be easily manageable\n",
    "data = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62010\n",
      "[((u'Thousands', u'NNS'), u'O'), ((u'of', u'IN'), u'O'), ((u'demonstrators', u'NNS'), u'O'), ((u'have', u'VBP'), u'O'), ((u'marched', u'VBN'), u'O'), ((u'through', u'IN'), u'O'), ((u'London', u'NNP'), u'B-geo'), ((u'to', u'TO'), u'O'), ((u'protest', u'VB'), u'O'), ((u'the', u'DT'), u'O'), ((u'war', u'NN'), u'O'), ((u'in', u'IN'), u'O'), ((u'Iraq', u'NNP'), u'B-geo'), ((u'and', u'CC'), u'O'), ((u'demand', u'VB'), u'O'), ((u'the', u'DT'), u'O'), ((u'withdrawal', u'NN'), u'O'), ((u'of', u'IN'), u'O'), ((u'British', u'JJ'), u'B-gpe'), ((u'troops', u'NNS'), u'O'), ((u'from', u'IN'), u'O'), ((u'that', u'DT'), u'O'), ((u'country', u'NN'), u'O'), ((u'.', u'.'), u'O')]\n"
     ]
    }
   ],
   "source": [
    "print len(data)\n",
    "print data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Thousands', u'NNS')    1354149\n",
      "O    1354149\n"
     ]
    }
   ],
   "source": [
    "# Split the data from their lables and flatten the lists. The size of the dataset is 1.354.149 samples\n",
    "datas = [item[k][0] for item in data for k in range(len(item))]\n",
    "labels = [item[k][1] for item in data for k in range(len(item))]\n",
    "print datas[0], \"  \", len(datas)\n",
    "print labels[0], \"  \", len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 1083319\n",
      "#test samples = 270830\n"
     ]
    }
   ],
   "source": [
    "# Split the above samples into training and test set to evaluate the training of Naive Bayes classifier\n",
    "training_samples = datas[:int(len(datas) * 0.8)]\n",
    "training_labels = labels[:int(len(datas) * 0.8)]\n",
    "test_samples = datas[int(len(datas) * 0.8):]\n",
    "test_labels = labels[int(len(datas) * 0.8):]\n",
    " \n",
    "print \"#training samples = %s\" % len(training_samples)    \n",
    "print \"#test samples = %s\" % len(test_samples)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into pandas dataframes to be able to transform them into an input that the classifier 'understands' \n",
    "df_training = pd.DataFrame(training_samples)\n",
    "df_training_labels = pd.DataFrame(training_labels)\n",
    "df_test = pd.DataFrame(test_samples)\n",
    "df_test_labels = pd.DataFrame(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the LabelEncoder to trasform the labels into numeric values for the classifier\n",
    "for column in df_training_labels.columns:\n",
    "    if df_training_labels[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        df_training_labels[column] = le.fit_transform(df_training_labels[column])\n",
    "\n",
    "for column in df_test_labels.columns:\n",
    "    if df_test_labels[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        df_test_labels[column] = le.fit_transform(df_test_labels[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model, fit the training data and make predictions on the test samples\n",
    "gnb = GaussianNB()\n",
    "data_fit = gnb.fit(df_training, np.array(df_training_labels).ravel())\n",
    "prediction = data_fit.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8270391020197172"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the classifier by accuracy\n",
    "accuracy_score(prediction, df_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
